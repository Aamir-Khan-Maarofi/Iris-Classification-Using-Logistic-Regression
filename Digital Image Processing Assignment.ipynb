{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: 1 -- Classify Iris using Logistic Regression   \n",
    "__Study the logistic regression model in scikit-learn and use it to classify the Iris data. List the python, scikit-learn commands that you use to apply the logistic model to the Iris data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importing libraries\n",
    "__Sklearn:__ Will use sklearn to import 'iris dataset', 'train_test_split' and 'LogisticRegression' and 'accuracy_socre' \n",
    "__NumPy:__ Will use it's unique(list/numpy array) method to find distinct values in a list or numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Loading and Understanding Data   \n",
    "We will use the load_iris() from sklearn to load the dataset, __load_iris__ returns data as Bunch object (sklearn custom object that extends dictionary and allow to access values by keys or attributes. e.g bunch['value_key'] or bunch.value_key). The bunch object returned from load_iris() has six feaatures that are; data, target, target_names, DESCR, feature_names, filename, frame. Following code cell look into the data returned by load_iris() as bunch object. With this we will get useful insights e.g; what are the types of data ? what is shape of features ? what is the target vector shape ? How many features ? How many training examples ? etc  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of iris_data:  <class 'sklearn.utils.Bunch'>\n",
      "Description:  .. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n",
      "iris_data keys:  dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
      "Shape of feature matrix:  (150, 4)\n",
      "Shape of target vector:  (150,)\n",
      "Features names:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "There are  [0 1 2]  distinct classes the target may fall in.\n",
      "Names of target classes:  ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Loading the data using load_iris() imported from sklearn\n",
    "iris_data = load_iris()\n",
    "\n",
    "# Checking what is the type of iris_data\n",
    "print('Type of iris_data: ', type(iris_data)) # Bunch object \n",
    "\n",
    "# Description of the data \n",
    "print('Description: ', iris_data.DESCR)\n",
    "print()\n",
    "\n",
    "# As bunch object inherit dectionary thus is must have keys() method to retrieve keys\n",
    "print('iris_data keys: ', iris_data.keys())\n",
    "\n",
    "# Checking dimension of feature matrix\n",
    "print('Shape of feature matrix: ', iris_data.data.shape)\n",
    "\n",
    "# Checking shape of target vector\n",
    "print('Shape of target vector: ', iris_data.target.shape)\n",
    "\n",
    "# Checking names of features\n",
    "print('Features names: ', iris_data.feature_names)\n",
    "\n",
    "# Checking how many classes does target vector have\n",
    "print('There are ', np.unique(iris_data.target), ' distinct classes the target may fall in.')\n",
    "\n",
    "# Cheking the names of distinct classes \n",
    "print('Names of target classes: ', iris_data.target_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Set Characteristics:**  \n",
    "After looking into data we determine that the data holds following characteristics\n",
    "\n",
    "    :Number of Instances: 150 (50 in each of three classes)\n",
    "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
    "    :Attribute Information:\n",
    "        - sepal length in cm\n",
    "        - sepal width in cm\n",
    "        - petal length in cm\n",
    "        - petal width in cm\n",
    "        - class:\n",
    "                - Iris-Setosa\n",
    "                - Iris-Versicolour\n",
    "                - Iris-Virginica\n",
    "     :Summary Statistics:\n",
    "\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "                    Min  Max   Mean    SD   Class Correlation\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
    "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
    "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
    "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "    :Class Distribution: 33.3% for each of 3 classes.\n",
    "**iris_data keys:**  dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])  \n",
    "**Shape of feature matrix:**  (150, 4)  \n",
    "**Shape of target vector:**  (150,)  \n",
    "**Features names:**  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']  \n",
    "**There are  [0 1 2]  distinct classes the target may fall in.**  \n",
    "**Names of target classes:**  ['setosa' 'versicolor' 'virginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Seperating Training and Testing Data\n",
    "Before we build machine learning model from our data we must consider how to test our model predictions on new measurements of iris flower. From load_iris() we have 150 samples that we would not use all for training because we would not use same data to evolovate it. This is because our model can always simply remember the whole training set, and will therefore always predict the correct label for any sample in the dataset. This will not indicate whether our model will 'generalize' to new measurements.\n",
    "\n",
    "To resolve this issue, we need to split dataset in training and testing set and best way to divide is to shuffle the samples first and then divide to 75% Training set and 25% Testing set. This way both sets will have all three species and will be exclusive.\n",
    "\n",
    "scikit-learn contains a function that shuffles the dataset and splits to training and testing set. The train_test_split() function shuffle dataset and extract 75% of the rows of data as training set with corresponding lables and 25% together with labels is declared as testing set.\n",
    "\n",
    "Arguments: data and target from iris_dataset dictionary and provided '0' as fixed seed to pesudorandom number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_data['data'], iris_data['target'], random_state = 0)\n",
    "\n",
    "# X_train, y_train represent the training features and targets, and X_test, y_test represent the test features and targets\n",
    "# O indicate that anytime the test_train_split is called with same data and targets they training and testing datasets \n",
    "# produced will be same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm whether data is splitted with exact percentages, following code is to find X_train, X_test, y_train and y_test dimensions, along with the iris_data.data and iris_data.target dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris_data feature matrix shape: (150, 4)\n",
      "Iris_data target vector shape: (150,)\n",
      "\n",
      "X_train shape: (112, 4)\n",
      "y_train shape: (112,)\n",
      "\n",
      "X_test shape: (38, 4)\n",
      "y_test shape: (38,)\n"
     ]
    }
   ],
   "source": [
    "print('Iris_data feature matrix shape: {}'.format(iris_data.data.shape))\n",
    "print('Iris_data target vector shape: {}'.format(iris_data.target.shape))\n",
    "\n",
    "print()\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "\n",
    "print()\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training the Model \n",
    "Will use LogisticRegression from sklearn to trian the model on iris_data. For training we will use the X_train as feature matrix an y_train as it's corresponding target vector. The fit method in LogisticRegression is used to  train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=190)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our own  instance o logisticRegression with maximum ieration for the solver to convergeis 190 \n",
    "logistic_model = LogisticRegression(max_iter = 190)\n",
    "# Fit method to train the model \n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Testing the Predictor  \n",
    "To test the predictor we have used predict() method, invoked with X_test. At the end using accuracy_sore, we have compared the y_test with the prediction made by the tranined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n",
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Predicting targets using X_test\n",
    "predictions = logistic_model.predict(X_test)\n",
    "print(predictions)\n",
    "\n",
    "# Test the model accury by comparing y_test and the predictions made in last statement.\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Making Predictions  on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81676472 9.50685805 0.51732652 2.4185819 ]\n",
      " [2.0860781  9.70280773 9.57216921 0.29165695]\n",
      " [5.1421919  4.28920452 0.21262029 5.35225571]\n",
      " [4.24670875 5.37444729 2.66515665 7.19899658]\n",
      " [1.40351833 8.57270618 6.20913169 0.03022408]]\n",
      "[0 2 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Generating numpy array of 5 rows random values\n",
    "new_data = np.random.rand(5, 4) * 10\n",
    "print(new_data)\n",
    "\n",
    "prediction = logistic_model.predict(new_data)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
